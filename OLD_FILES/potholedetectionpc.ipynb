{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:03.689022Z",
     "start_time": "2024-12-30T21:15:55.832569Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:09.146836Z",
     "start_time": "2024-12-30T21:16:03.704979Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:10.864020Z",
     "start_time": "2024-12-30T21:16:09.509911Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:15.394899Z",
     "start_time": "2024-12-30T21:16:10.892947Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:15.442288Z",
     "start_time": "2024-12-30T21:16:15.428326Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:15.550512Z",
     "start_time": "2024-12-30T21:16:15.529569Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # This will show the CUDA version that PyTorch was built with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:15.672704Z",
     "start_time": "2024-12-30T21:16:15.654752Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Verify PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:16:15.763976Z",
     "start_time": "2024-12-30T21:16:15.742036Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:14:28.002667Z",
     "start_time": "2024-12-30T21:14:27.992172Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_md9cK9QJLs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:37:39.676843Z",
     "start_time": "2024-12-12T19:37:37.973564Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14793,
     "status": "ok",
     "timestamp": 1733766453290,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "kOBNHEbozuFW",
    "outputId": "49840750-75b6-487c-9b51-595aebeb5e80"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:44:43.731086Z",
     "start_time": "2024-12-10T12:44:43.521039Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43988,
     "status": "ok",
     "timestamp": 1733766497273,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "HApn648Rz35U",
    "outputId": "a2ae4b99-edf6-4e40-d984-d32de1c7f01d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:05:11.152334Z",
     "start_time": "2024-12-10T13:05:11.146516Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace with the name of your folder\n",
    "folder_name = 'dataset/train/images'\n",
    "\n",
    "# Get the absolute path\n",
    "absolute_path = os.path.abspath(folder_name)\n",
    "print(f\"Absolute path: {absolute_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T18:40:16.307294Z",
     "start_time": "2024-12-26T18:40:16.296811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1733660211739,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "3qaFeiHx-9lL",
    "outputId": "0c98962d-79a9-4009-9eb5-18ca56eb0a18"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for images and labels\n",
    "image_folder = r'archive\\dataset\\dataset\\train\\images'\n",
    "label_folder = r'archive\\dataset\\dataset\\train\\labels'\n",
    "# List the image and label files\n",
    "image_files = os.listdir(image_folder)\n",
    "label_files = os.listdir(label_folder)\n",
    "\n",
    "# Count the current number of images and labels\n",
    "num_images = len(image_files)\n",
    "num_labels = len(label_files)\n",
    "\n",
    "num_images, num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T18:24:04.015150Z",
     "start_time": "2024-12-26T18:24:03.177350Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = r'archive\\dataset\\dataset\\train\\images'\n",
    "\n",
    "# List all .png files in the folder\n",
    "png_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "# Convert each .png to .jpg\n",
    "for png_file in png_files:\n",
    "    # Construct full file path\n",
    "    png_path = os.path.join(image_folder, png_file)\n",
    "    jpg_path = os.path.join(image_folder, os.path.splitext(png_file)[0] + '.jpg')\n",
    "\n",
    "    # Open the .png image and convert it to .jpg\n",
    "    with Image.open(png_path) as img:\n",
    "        rgb_img = img.convert('RGB')  # Ensure the image is in RGB mode\n",
    "        rgb_img.save(jpg_path, 'JPEG')\n",
    "\n",
    "    # Optionally, delete the original .png file\n",
    "    os.remove(png_path)\n",
    "    print(f\"Converted {png_file} to {os.path.basename(jpg_path)} and deleted the original.\")\n",
    "\n",
    "print(\"Conversion completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T18:36:07.958446Z",
     "start_time": "2024-12-26T18:36:07.942339Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for images and labels\n",
    "image_folder = r'archive\\dataset\\dataset\\train\\images'\n",
    "label_folder = r'archive\\dataset\\dataset\\train\\labels'\n",
    "\n",
    "# List the image and label files (filter by extensions)\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "label_files = [f for f in os.listdir(label_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Extract base names without extensions for comparison\n",
    "image_basenames = {os.path.splitext(file)[0] for file in image_files}\n",
    "label_basenames = {os.path.splitext(file)[0] for file in label_files}\n",
    "\n",
    "# Find the images that don't have corresponding labels\n",
    "unmatched_images = image_basenames - label_basenames\n",
    "\n",
    "# Delete unmatched images\n",
    "for image_name in unmatched_images:\n",
    "    # Construct the full file path\n",
    "    for extension in ['.jpg', '.png', '.jpeg']:\n",
    "        image_path = os.path.join(image_folder, image_name + extension)\n",
    "        if os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "            print(f\"Deleted unmatched image: {image_path}\")\n",
    "\n",
    "# Print completion message\n",
    "print(\"Unmatched images deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potholes66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3544,
     "status": "ok",
     "timestamp": 1733087629665,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "JE6Udtv7Tf37",
    "outputId": "1b56c7ac-998b-41b0-f3f9-526c5f2671b4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths to the folders for each category\n",
    "pothole_folder = 'archive/dataset/dataset/classes/pothole/images'\n",
    "crack_folder = 'archive/dataset/dataset/classes/cracks/images'\n",
    "open_manhole_folder = 'archive/dataset/dataset/classes/open_manhole/images'\n",
    "\n",
    "# Function to count image files in a folder\n",
    "def count_images(folder, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return len([f for f in os.listdir(folder) if f.endswith(extensions)])\n",
    "\n",
    "# Count images in each category\n",
    "pothole_count = count_images(pothole_folder)\n",
    "crack_count = count_images(crack_folder)\n",
    "open_manhole_count = count_images(open_manhole_folder)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of pothole images: {pothole_count}\")\n",
    "print(f\"Number of crack images: {crack_count}\")\n",
    "print(f\"Number of open manhole images: {open_manhole_count}\")\n",
    "\n",
    "# Total count\n",
    "total_images = pothole_count + crack_count + open_manhole_count\n",
    "print(f\"Total number of images: {total_images}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124917,
     "status": "ok",
     "timestamp": 1732863060624,
     "user": {
      "displayName": "Zurin",
      "userId": "09903489326107450528"
     },
     "user_tz": -360
    },
    "id": "bGrFphNDVscQ",
    "outputId": "adcce601-c8e3-4108-cbbd-85a8029f545a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths for images and labels\n",
    "images_path = 'archive/dataset/dataset/classes/cracks/images'\n",
    "labels_path = 'archive/dataset/dataset/classes/cracks/labels/txt'\n",
    "output_images_path = 'archive/dataset/dataset/classes/cracks/aug_date/images'\n",
    "output_labels_path = 'archive/dataset/dataset/classes/cracks/aug_date/labels'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_images_path, exist_ok=True)\n",
    "os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "# Transformation pipeline with flipping\n",
    "augment = A.Compose(\n",
    "    [\n",
    "        A.Rotate(limit=30, border_mode=0, p=1.0),               # Rotate within ±30 degrees\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3,\n",
    "                                    contrast_limit=0.3, p=1.0), # Adjust brightness\n",
    "        A.HorizontalFlip(p=0.5),                                # Random horizontal flip\n",
    "        A.VerticalFlip(p=0.5),                                  # Random vertical flip\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids'])\n",
    ")\n",
    "\n",
    "# 只抓 .jpg/.png 跟 .txt\n",
    "image_files = sorted([f for f in os.listdir(images_path)\n",
    "                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "label_files  = sorted([f for f in os.listdir(labels_path)\n",
    "                      if f.lower().endswith('.txt')])\n",
    "\n",
    "# Initial image count\n",
    "current_image_count = len(image_files)\n",
    "target_image_count = 500\n",
    "new_image_id = current_image_count + 1\n",
    "\n",
    "# Loop through existing images and augment\n",
    "for image_file, label_file in tqdm(zip(image_files, label_files), total=len(image_files)):\n",
    "    if current_image_count >= target_image_count:\n",
    "        break\n",
    "\n",
    "    # Load image and corresponding label\n",
    "    image_path = os.path.join(images_path, image_file)\n",
    "    label_path = os.path.join(labels_path, label_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Load bounding boxes from YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        category_ids.append(int(class_id))\n",
    "\n",
    "    # Apply augmentation\n",
    "    augmented = augment(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "\n",
    "    # Save augmented image\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented['image'])\n",
    "\n",
    "    # Save augmented labels\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented['bboxes'], augmented['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    # Update counters\n",
    "    new_image_id += 1\n",
    "    current_image_count += 1\n",
    "\n",
    "print(f\"Augmentation completed! Total images: {current_image_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20904,
     "status": "ok",
     "timestamp": 1732647868287,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "MCJ7qTLEaNq2",
    "outputId": "ecdc9222-cb3d-4597-f954-5f32697b8f65"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths for images and labels\n",
    "images_path = 'archive/dataset/dataset/classes/cracks/images'\n",
    "labels_path = 'archive/dataset/dataset/classes/cracks/labels'\n",
    "output_images_path = 'archive/dataset/dataset/classes/cracks/aug_date/images'\n",
    "output_labels_path = 'archive/dataset/dataset/classes/cracks/aug_date/labels'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_images_path, exist_ok=True)\n",
    "os.makedirs(output_labels_path, exist_ok=True)  # Only need to create the directories once\n",
    "\n",
    "# Transformation pipeline without any augmentation (we'll apply them manually)\n",
    "rotate_augment = A.Compose([A.Rotate(limit=30, border_mode=0, p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "horizontal_flip_augment = A.Compose([A.HorizontalFlip(p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "vertical_flip_augment = A.Compose([A.VerticalFlip(p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "brightness_contrast_augment = A.Compose([A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "\n",
    "# Get list of all images and labels\n",
    "# 只抓 .jpg/.png 跟 .txt\n",
    "image_files = sorted([f for f in os.listdir(images_path)\n",
    "                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "label_files  = sorted([f for f in os.listdir(labels_path)\n",
    "                      if f.lower().endswith('.txt')])\n",
    "\n",
    "# Initial image count\n",
    "current_image_count = 667  # Start from 667 (your current count)\n",
    "max_image_count = 1000\n",
    "new_image_id = current_image_count\n",
    "\n",
    "# Loop through existing images and augment\n",
    "for image_file, label_file in tqdm(zip(image_files, label_files), total=len(image_files)):\n",
    "    if new_image_id > max_image_count:\n",
    "        break\n",
    "\n",
    "    # Load image and corresponding label\n",
    "    image_path = os.path.join(images_path, image_file)\n",
    "    label_path = os.path.join(labels_path, label_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Load bounding boxes from YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        category_ids.append(int(class_id))\n",
    "\n",
    "    # Apply augmentations separately\n",
    "    aug_image = image.copy()\n",
    "\n",
    "    # Apply rotation\n",
    "    augmented_rotate = rotate_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_rotate['image'])\n",
    "\n",
    "    # Apply horizontal flip\n",
    "    augmented_flip = horizontal_flip_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_flip['image'])\n",
    "\n",
    "    # Apply vertical flip\n",
    "    augmented_vertical_flip = vertical_flip_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_vertical_flip['image'])\n",
    "\n",
    "    # Apply brightness and contrast\n",
    "    augmented_brightness = brightness_contrast_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_brightness['image'])\n",
    "\n",
    "    # Save augmented labels\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_rotate['bboxes'], augmented_rotate['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_flip['bboxes'], augmented_flip['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_vertical_flip['bboxes'], augmented_vertical_flip['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_brightness['bboxes'], augmented_brightness['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    # Update counters\n",
    "    new_image_id += 1\n",
    "\n",
    "print(f\"Augmentation completed! Total images: {new_image_id - 667}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9282,
     "status": "ok",
     "timestamp": 1732648026250,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "SECSQMtB3lY6",
    "outputId": "e6dd095f-a33b-4f53-d395-790d72fce45e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths for images and labels\n",
    "images_path = 'archive/dataset/dataset/classes/cracks/images'\n",
    "labels_path = 'archive/dataset/dataset/classes/cracks/labels'\n",
    "output_images_path = 'archive/dataset/dataset/classes/cracks/aug_date/images'\n",
    "output_labels_path = 'archive/dataset/dataset/classes/cracks/aug_date/labels'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_images_path, exist_ok=True)\n",
    "os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "# Transformation pipeline with flipping\n",
    "augment = A.Compose(\n",
    "    [\n",
    "        A.Rotate(limit=30, border_mode=0, p=1.0),               # Rotate within ±30 degrees\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3,\n",
    "                                    contrast_limit=0.3, p=1.0), # Adjust brightness\n",
    "        A.HorizontalFlip(p=0.5),                                # Random horizontal flip\n",
    "        A.VerticalFlip(p=0.5),                                  # Random vertical flip\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids'])\n",
    ")\n",
    "\n",
    "# Get list of all images and labels\n",
    "# 只抓 .jpg/.png 跟 .txt\n",
    "image_files = sorted([f for f in os.listdir(images_path)\n",
    "                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "label_files  = sorted([f for f in os.listdir(labels_path)\n",
    "                      if f.lower().endswith('.txt')])\n",
    "\n",
    "# Initial image count (starting from 0833)\n",
    "current_image_count = 832  # Your last image is 0832\n",
    "new_image_id = 833  # Start the new numbering from 0833\n",
    "\n",
    "# Loop through existing images and augment\n",
    "for image_file, label_file in tqdm(zip(image_files, label_files), total=len(image_files)):\n",
    "\n",
    "    # Load image and corresponding label\n",
    "    image_path = os.path.join(images_path, image_file)\n",
    "    label_path = os.path.join(labels_path, label_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Load bounding boxes from YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        category_ids.append(int(class_id))\n",
    "\n",
    "    # Apply augmentation\n",
    "    augmented = augment(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "\n",
    "    # Save augmented image with numeric naming convention\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{new_image_id:04}.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented['image'])\n",
    "\n",
    "    # Save augmented labels with numeric naming convention\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{new_image_id:04}.txt')\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented['bboxes'], augmented['category_ids']):\n",
    "            f.write(f\"{category_id} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    # Update counters\n",
    "    new_image_id += 1\n",
    "\n",
    "    if new_image_id > 1000:  # Stop once the image count reaches 1000\n",
    "        break\n",
    "\n",
    "print(f\"Augmentation completed! Total images: {new_image_id - 833}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1733088021864,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "KNV7UCB90gTw",
    "outputId": "dd9c3548-8330-452f-982e-7e654bdb1c9b"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1733088024150,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "WRH3DzKf0pjs",
    "outputId": "aa8cfa00-3483-4c37-aa37-5af2ccb6cd15"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWvn1QMTp0cm"
   },
   "source": [
    "# Model Trainning YOLOv8s **(don't touch it)** (bullshit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de95GvnxqVSF"
   },
   "source": [
    "## Results are fetched from dataset, Model created pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stupid pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1261,
     "status": "ok",
     "timestamp": 1733158183828,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "kMLJBboy92Fh",
    "outputId": "40c91082-783e-4d63-f33a-a16043f2738c"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/data/runs/detect/train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "executionInfo": {
     "elapsed": 1481,
     "status": "ok",
     "timestamp": 1733158201632,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "Jhu2rvlN-2Es",
    "outputId": "e3fc103f-94a6-45d5-a958-d7263ad0b2c6"
   },
   "outputs": [],
   "source": [
    "Image(filename='/content/drive/MyDrive/data/runs/detect/train3/results.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1733158193473,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "7mTIRFr2eJXj",
    "outputId": "4d90d387-0b85-48eb-869a-f4358096d78b"
   },
   "outputs": [],
   "source": [
    "Image(filename='/content/drive/MyDrive/data/runs/detect/train3/F1_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 1846,
     "status": "ok",
     "timestamp": 1732703193777,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "LsGSt3hRdiUf",
    "outputId": "d16bee61-89e1-47fb-d6f0-f91433af1e9c"
   },
   "outputs": [],
   "source": [
    "Image(filename='/content/drive/MyDrive/data/runs/detect/train/P_curve.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 3085,
     "status": "ok",
     "timestamp": 1733158319383,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "zNAH5ANH--P1",
    "outputId": "5083a65a-4d40-472d-8ccf-3fbc67337558"
   },
   "outputs": [],
   "source": [
    "Image(filename='/content/drive/MyDrive/data/runs/detect/train3/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command line validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15662,
     "status": "ok",
     "timestamp": 1732703239285,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "GfthqbKmBRuv",
    "outputId": "b5a3e5cd-b8aa-4159-e75b-02a80c733b56"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=val model=/content/drive/MyDrive/data/runs/detect/train/weights/best.pt data=data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18579,
     "status": "ok",
     "timestamp": 1732703992465,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "Z_Zb1IExBlWy",
    "outputId": "578d1ceb-d7d4-4b7e-e043-93167193694c"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=/content/drive/MyDrive/data/runs/detect/train/weights/best.pt conf=0.25 source=archive/dataset/dataset/test/images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwalgcNZq6M-"
   },
   "source": [
    "Test data's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "executionInfo": {
     "elapsed": 9651,
     "status": "ok",
     "timestamp": 1732997066270,
     "user": {
      "displayName": "soad islam",
      "userId": "10688171057968505430"
     },
     "user_tz": 480
    },
    "id": "0YK0r8gUNN1E",
    "outputId": "6f337516-0c1f-4537-88bf-16cf3017efa3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder where YOLO saved the results\n",
    "results_path = \"runs/detect/predict\"\n",
    "\n",
    "# Get the list of predicted images\n",
    "predicted_images = sorted(\n",
    "    [os.path.join(results_path, img) for img in os.listdir(results_path) if img.endswith(('.jpg', '.png'))]\n",
    ")\n",
    "\n",
    "# Set up the plot (1 row and as many columns as needed)\n",
    "fig, axes = plt.subplots(1, len(predicted_images), figsize=(20, 5))\n",
    "\n",
    "# Loop through the images and display them\n",
    "for i, image_path in enumerate(predicted_images):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "    # Display the image in the row\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')  # Hide axes\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd9hBrpYtFKd"
   },
   "source": [
    "### **Result Show:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1732997205873,
     "user": {
      "displayName": "soad islam",
      "userId": "10688171057968505430"
     },
     "user_tz": 480
    },
    "id": "MlDlow8oZ4zp",
    "outputId": "c137e730-52b8-429d-e2d2-896c918266fc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the image from your local machine\n",
    "uploaded = files.upload()  # This will prompt you to upload an image\n",
    "\n",
    "# Get the uploaded image file path\n",
    "image_path = list(uploaded.keys())[0]\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO('/content/drive/MyDrive/data/runs/detect/train/weights/best.pt')\n",
    "\n",
    "# Perform prediction on the uploaded image\n",
    "results = model(image_path)\n",
    "\n",
    "# Extract the image with bounding boxes\n",
    "img_with_boxes = results[0].plot()  # Plot the image with boxes around detected objects\n",
    "\n",
    "# Display the image with bounding boxes using matplotlib\n",
    "plt.imshow(img_with_boxes)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0sZTv_ytEoU"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the path to the prediction results\n",
    "predicted_images_path = 'runs/detect/predict'  # This is the default output folder\n",
    "\n",
    "# List all images with .jpg and .jpeg extensions in the prediction folder\n",
    "predicted_images = glob.glob(predicted_images_path + '/*.jpg') + glob.glob(predicted_images_path + '/*.jpeg')\n",
    "\n",
    "# Display each detected image with a smaller size\n",
    "for img_path in predicted_images:\n",
    "    print(f\"Detected image: {img_path}\")\n",
    "    display(Image(filename=img_path, width=300))  # Set width to 300px (adjust as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11499,
     "status": "ok",
     "timestamp": 1733327521055,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "n0fLIMPpfwHp",
    "outputId": "55025ecf-c507-4960-9bf9-105ea8d241bf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for your train and valid folders\n",
    "base_dir = 'archive'\n",
    "train_path = os.path.join(base_dir, 'dataset', 'dataset', 'train', 'images')\n",
    "valid_path = os.path.join(base_dir, 'dataset', 'dataset', 'valid', 'images')\n",
    "\n",
    "# Count the number of files in each folder\n",
    "train_files = len(os.listdir(train_path))\n",
    "valid_files = len(os.listdir(valid_path))\n",
    "\n",
    "# Calculate the total number of files\n",
    "total_files = train_files + valid_files\n",
    "\n",
    "# Calculate percentages\n",
    "train_percentage = (train_files / total_files) * 100\n",
    "valid_percentage = (valid_files / total_files) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Data: {train_files} images ({train_percentage:.2f}%)\")\n",
    "print(f\"Validation Data: {valid_files} images ({valid_percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25376,
     "status": "ok",
     "timestamp": 1733346833085,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "Djulb23oSd2a",
    "outputId": "95cfff4b-3824-4f0d-ab45-5b91a84c44df"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=/content/drive/MyDrive/data/runs/detect/train9/weights/best.pt conf=0.25 source=/content/drive/MyDrive/data/dataset/test/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 4068,
     "status": "ok",
     "timestamp": 1733346844542,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "Fp8myQVdSnUQ",
    "outputId": "8a6c338e-25c2-49fb-8f4f-e9ddfb1be3fe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder where YOLO saved the results\n",
    "results_path = \"runs/detect/predict\"\n",
    "\n",
    "# Get the list of predicted images\n",
    "predicted_images = sorted(\n",
    "    [os.path.join(results_path, img) for img in os.listdir(results_path) if img.endswith(('.jpg', '.png'))]\n",
    ")\n",
    "\n",
    "# Set up the plot (1 row and as many columns as needed)\n",
    "fig, axes = plt.subplots(1, len(predicted_images), figsize=(20, 5))\n",
    "\n",
    "# Loop through the images and display them\n",
    "for i, image_path in enumerate(predicted_images):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "    # Display the image in the row\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')  # Hide axes\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwhGW0nzTH5E",
    "outputId": "e552aa16-b8e3-49da-815f-edf14040320f"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from IPython.display import Video  # For displaying the video in Colab\n",
    "\n",
    "# Load your YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/data/runs/detect/train/weights/best.pt')\n",
    "\n",
    "# Path to your input video\n",
    "video_path = '/content/drive/MyDrive/data/dataset/test_video/testvideo1.mp4'  # Replace with your video file path\n",
    "output_path = '/content/drive/MyDrive/data/predicted/train9_video1.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save the annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Annotate the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Display the processed video in Colab\n",
    "print(f\"Processed video saved at: {output_path}\")\n",
    "Video(output_path, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "10HrUYol439VIorrXdFpddW835FJo5Ui6"
    },
    "id": "6dXWlYiZ_pof",
    "outputId": "efbb89a4-8d5f-4338-9361-c208197b58c6"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from IPython.display import Video  # For displaying the video in Colab\n",
    "\n",
    "# Load your YOLO model\n",
    "model = YOLO('/content/drive/MyDrive/data/runs/detect/train20/weights/best.pt')\n",
    "\n",
    "# Path to your input video\n",
    "video_path = '/content/drive/MyDrive/data/dataset/test_video/testvideo1.mp4'  # Replace with your video file path\n",
    "output_path = '/content/drive/MyDrive/data/predicted/train20_video1.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save the annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Annotate the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Display the processed video in Colab\n",
    "print(f\"Processed video saved at: {output_path}\")\n",
    "Video(output_path, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8105,
     "status": "ok",
     "timestamp": 1733669081679,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "8LZfI9nqcosq",
    "outputId": "6923b9c1-2d0f-4313-80a6-2b8dd3af5f1e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder where YOLO saved the results\n",
    "results_path = \"runs/detect/predict7\"\n",
    "\n",
    "# Get the list of predicted images\n",
    "predicted_images = sorted(\n",
    "    [os.path.join(results_path, img) for img in os.listdir(results_path) if img.endswith(('.jpg', '.png'))]\n",
    ")\n",
    "\n",
    "# Set the number of rows and columns\n",
    "rows, columns = 4, 5  # 4 rows and 5 columns for 20 images\n",
    "\n",
    "# Adjust the figure size for better visibility\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20, 15))\n",
    "\n",
    "# Loop through the images and display them\n",
    "for i, image_path in enumerate(predicted_images):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "    # Get the corresponding subplot\n",
    "    ax = axes[i // columns, i % columns]\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(predicted_images), rows * columns):\n",
    "    axes[i // columns, i % columns].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24138,
     "status": "ok",
     "timestamp": 1733245245888,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "2yWey3WxQ9R5",
    "outputId": "7f3048c9-4fee-4a43-cd30-b19b33eedb2b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths for images and labels\n",
    "images_path = '/content/drive/MyDrive/data/dataset/classes/cracks/images'\n",
    "labels_path = '/content/drive/MyDrive/data/dataset/classes/cracks/labels/txt'\n",
    "output_images_path = '/content/drive/MyDrive/data/dataset/classes/cracks/augmented/images'\n",
    "output_labels_path = '/content/drive/MyDrive/data/dataset/classes/cracks/augmented/labels'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_images_path, exist_ok=True)\n",
    "os.makedirs(output_labels_path, exist_ok=True)  # Ensure the output labels directory exists\n",
    "\n",
    "# Transformation pipeline without any augmentation (we'll apply them manually)\n",
    "rotate_augment = A.Compose([A.Rotate(limit=30, border_mode=0, p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "horizontal_flip_augment = A.Compose([A.HorizontalFlip(p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "vertical_flip_augment = A.Compose([A.VerticalFlip(p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "brightness_contrast_augment = A.Compose([A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0)], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))\n",
    "\n",
    "# Get list of all images and labels\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "# Loop through existing images and augment\n",
    "for image_file, label_file in tqdm(zip(image_files, label_files), total=len(image_files)):\n",
    "    # Load image and corresponding label\n",
    "    image_path = os.path.join(images_path, image_file)\n",
    "    label_path = os.path.join(labels_path, label_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Load bounding boxes from YOLO label file\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    bboxes = []\n",
    "    category_ids = []\n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "        bboxes.append([x_center, y_center, width, height])\n",
    "        category_ids.append(int(class_id))  # Ensure class_id is an integer\n",
    "\n",
    "    # Apply augmentations separately\n",
    "    aug_image = image.copy()\n",
    "\n",
    "    # Apply rotation\n",
    "    augmented_rotate = rotate_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    base_name = os.path.splitext(image_file)[0]\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{base_name}_aug_rotate.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_rotate['image'])\n",
    "\n",
    "    # Save augmented label for rotated image\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{base_name}_aug_rotate.txt')\n",
    "    os.makedirs(os.path.dirname(augmented_label_path), exist_ok=True)  # Ensure directory exists\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_rotate['bboxes'], augmented_rotate['category_ids']):\n",
    "            f.write(f\"{int(category_id)} {' '.join(map(str, bbox))}\\n\")  # Ensure class_id is saved as an integer\n",
    "\n",
    "    # Apply horizontal flip\n",
    "    augmented_flip = horizontal_flip_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{base_name}_aug_flip.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_flip['image'])\n",
    "\n",
    "    # Save augmented label for horizontal flip\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{base_name}_aug_flip.txt')\n",
    "    os.makedirs(os.path.dirname(augmented_label_path), exist_ok=True)  # Ensure directory exists\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_flip['bboxes'], augmented_flip['category_ids']):\n",
    "            f.write(f\"{int(category_id)} {' '.join(map(str, bbox))}\\n\")  # Ensure class_id is saved as an integer\n",
    "\n",
    "    # Apply vertical flip\n",
    "    augmented_vertical_flip = vertical_flip_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{base_name}_aug_vertical_flip.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_vertical_flip['image'])\n",
    "\n",
    "    # Save augmented label for vertical flip\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{base_name}_aug_vertical_flip.txt')\n",
    "    os.makedirs(os.path.dirname(augmented_label_path), exist_ok=True)  # Ensure directory exists\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_vertical_flip['bboxes'], augmented_vertical_flip['category_ids']):\n",
    "            f.write(f\"{int(category_id)} {' '.join(map(str, bbox))}\\n\")  # Ensure class_id is saved as an integer\n",
    "\n",
    "    # Apply brightness and contrast\n",
    "    augmented_brightness = brightness_contrast_augment(image=aug_image, bboxes=bboxes, category_ids=category_ids)\n",
    "    augmented_image_path = os.path.join(output_images_path, f'{base_name}_aug_brightness_contrast.jpg')\n",
    "    cv2.imwrite(augmented_image_path, augmented_brightness['image'])\n",
    "\n",
    "    # Save augmented label for brightness and contrast\n",
    "    augmented_label_path = os.path.join(output_labels_path, f'{base_name}_aug_brightness_contrast.txt')\n",
    "    os.makedirs(os.path.dirname(augmented_label_path), exist_ok=True)  # Ensure directory exists\n",
    "    with open(augmented_label_path, 'w') as f:\n",
    "        for bbox, category_id in zip(augmented_brightness['bboxes'], augmented_brightness['category_ids']):\n",
    "            f.write(f\"{int(category_id)} {' '.join(map(str, bbox))}\\n\")  # Ensure class_id is saved as an integer\n",
    "\n",
    "print(f\"Augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkriULqvZJWo"
   },
   "source": [
    "first augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1733676640737,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "F9khfD38RNLE",
    "outputId": "3da98e06-819f-40bb-885e-e8056d0b843e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directories for images and labels\n",
    "image_dir = '/content/drive/MyDrive/data/dataset/train/images'\n",
    "txt_dir = '/content/drive/MyDrive/data/dataset/train/labels'\n",
    "\n",
    "# Count number of image files and label files\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "label_files = [f for f in os.listdir(txt_dir) if os.path.isfile(os.path.join(txt_dir, f))]\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Number of image files: {len(image_files)}\")\n",
    "print(f\"Number of label files: {len(label_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8s.pt')  # Pre-trained YOLOv8 small model\n",
    "\n",
    "# Train the model with increased image size\n",
    "model.train(\n",
    "    data=\"C:/Users/Ahsan/Downloads/data/data.yaml\",  # Path to data.yaml\n",
    "    epochs=25,\n",
    "    imgsz=640,  # Increased image size\n",
    "    plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20082,
     "status": "ok",
     "timestamp": 1733688709147,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "V-9SEfVH5dj8",
    "outputId": "dfaaa187-1e7c-4fd8-9656-e70c3339a9be"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=/content/drive/MyDrive/data/runs/detect/train20/weights/best.pt conf=0.25 source=/content/drive/MyDrive/data/dataset/test/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847,
     "output_embedded_package_id": "1FEsiZlzwyKR9NNlecb8t9HNGn4m0q1Ci"
    },
    "executionInfo": {
     "elapsed": 9280,
     "status": "ok",
     "timestamp": 1733688741235,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "DcjMYLwA5ngL",
    "outputId": "b11bd89e-42bd-4d5a-81fd-95861273a30b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder where YOLO saved the results\n",
    "results_path = \"runs/detect/predict8\"\n",
    "\n",
    "# Get the list of predicted images\n",
    "predicted_images = sorted(\n",
    "    [os.path.join(results_path, img) for img in os.listdir(results_path) if img.endswith(('.jpg', '.png'))]\n",
    ")\n",
    "\n",
    "# Set the number of rows and columns\n",
    "rows, columns = 4, 5  # 4 rows and 5 columns for 20 images\n",
    "\n",
    "# Adjust the figure size for better visibility\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(20, 15))\n",
    "\n",
    "# Loop through the images and display them\n",
    "for i, image_path in enumerate(predicted_images):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "    # Get the corresponding subplot\n",
    "    ax = axes[i // columns, i % columns]\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    ax.set_title(f\"Image {i+1}\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(predicted_images), rows * columns):\n",
    "    axes[i // columns, i % columns].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4549,
     "status": "ok",
     "timestamp": 1733689986947,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "PDfI1PTD-R45",
    "outputId": "58e9a2e8-b348-4a56-855d-72e938f29682"
   },
   "outputs": [],
   "source": [
    "pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pls8YhMTMLys"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Class names from your data.yaml\n",
    "class_names = [\"pothole\", \"cracks\", \"open_manhole\"]\n",
    "\n",
    "# Function to convert YOLO annotations to COCO format\n",
    "def convert_yolo_to_coco(image_dir, label_dir, output_json):\n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    coco_categories = [{\"id\": i+1, \"name\": class_names[i]} for i in range(len(class_names))]\n",
    "    annotation_id = 1  # Start annotation ID from 1\n",
    "\n",
    "    # Process each image in the directory\n",
    "    for image_filename in os.listdir(image_dir):\n",
    "        if image_filename.endswith(\".jpg\"):  # Process only jpg images\n",
    "            image_path = os.path.join(image_dir, image_filename)\n",
    "            label_path = os.path.join(label_dir, os.path.splitext(image_filename)[0] + \".txt\")\n",
    "\n",
    "            if not os.path.exists(label_path):  # Skip if no label file exists for the image\n",
    "                continue\n",
    "\n",
    "            # Get image info for COCO format\n",
    "            image = cv2.imread(image_path)\n",
    "            height, width, _ = image.shape\n",
    "            image_id = len(coco_images) + 1\n",
    "\n",
    "            coco_images.append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_filename,\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            })\n",
    "\n",
    "            # Process each line in the label file\n",
    "            with open(label_path, \"r\") as label_file:\n",
    "                for line in label_file:\n",
    "                    # Parse YOLO annotation\n",
    "                    class_id, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "\n",
    "                    # Convert from normalized to pixel coordinates\n",
    "                    x_min = (x_center - w / 2) * width\n",
    "                    y_min = (y_center - h / 2) * height\n",
    "                    x_max = (x_center + w / 2) * width\n",
    "                    y_max = (y_center + h / 2) * height\n",
    "\n",
    "                    # Create COCO annotation (add 1 to class_id for COCO format)\n",
    "                    coco_annotation = {\n",
    "                        \"segmentation\": [],  # Add segmentation data if available\n",
    "                        \"area\": (x_max - x_min) * (y_max - y_min),  # Area of the bounding box\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                        \"category_id\": int(class_id) + 1,  # COCO category ID starts at 1\n",
    "                        \"id\": annotation_id\n",
    "                    }\n",
    "                    coco_annotations.append(coco_annotation)\n",
    "                    annotation_id += 1\n",
    "\n",
    "    # Create final COCO dataset\n",
    "    coco_data = {\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations,\n",
    "        \"categories\": coco_categories\n",
    "    }\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_json, \"w\") as output_file:\n",
    "        json.dump(coco_data, output_file, indent=4)\n",
    "\n",
    "# Directories for images and labels\n",
    "image_dir = '/content/drive/MyDrive/data/dataset/valid/images'\n",
    "label_dir = '/content/drive/MyDrive/data/dataset/valid/labels'\n",
    "\n",
    "# Output JSON file for COCO annotations\n",
    "output_json = '/content/drive/MyDrive/data/valid_annotations.json'\n",
    "\n",
    "# Convert YOLO to COCO\n",
    "convert_yolo_to_coco(image_dir, label_dir, output_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:17:10.963958Z",
     "start_time": "2024-12-26T20:17:10.087736Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7775,
     "status": "ok",
     "timestamp": 1733732414094,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "3Cq_vQflZLmO",
    "outputId": "dc4a1e6a-f9a2-41f0-b571-1712b779f9e8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Path to your COCO annotations JSON file\n",
    "coco_annotation_file = 'F:/data/valid_annotations1.json'\n",
    "\n",
    "\n",
    "# Initialize COCO API\n",
    "coco = COCO(coco_annotation_file)\n",
    "\n",
    "# Check the categories (class names)\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_names = [category['name'] for category in categories]\n",
    "print(f\"Categories: {category_names}\")\n",
    "\n",
    "# Define fixed colors for each category\n",
    "category_colors = {\n",
    "    \"pothole\": (255, 0, 0),        # Red for potholes\n",
    "    \"cracks\": (0, 255, 0),         # Green for cracks\n",
    "    \"open_manhole\": (0, 0, 255)    # Blue for open manholes\n",
    "}\n",
    "\n",
    "# Load image IDs\n",
    "image_ids = coco.getImgIds()  # Get all image IDs\n",
    "num_images_to_show = 30  # Number of random images to display\n",
    "\n",
    "# Shuffle the list of image IDs to display random images\n",
    "random.shuffle(image_ids)\n",
    "images_to_display = image_ids[:num_images_to_show]  # Select the first 30 random images\n",
    "\n",
    "# Set up the matplotlib figure to show multiple images\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, image_id in enumerate(images_to_display):\n",
    "    # Load the image\n",
    "    image_info = coco.loadImgs(image_id)[0]\n",
    "    image_path = 'F:/data/dataset/valid/images/' + image_info['file_name']\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load the annotations for this image\n",
    "    annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "    annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "    # Draw the bounding boxes on the image\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']  # The bounding box: [x, y, width, height]\n",
    "        category_id = ann['category_id']\n",
    "\n",
    "        # Adjust for YOLO category indexing (starts from 0 in YOLO, but starts from 1 in COCO)\n",
    "        if category_id > 0:\n",
    "            category_name = category_names[category_id - 1]\n",
    "        else:\n",
    "            category_name = category_names[category_id]  # Since YOLO starts from 0\n",
    "\n",
    "        # Get the fixed color for this category\n",
    "        color = category_colors[category_name]\n",
    "\n",
    "        # Draw bounding box and label with the assigned color\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
    "        cv2.putText(image, category_name, (int(x), int(y)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display the image with its bounding boxes\n",
    "    plt.subplot(6, 5, i + 1)  # 6 rows, 5 columns of images\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "\n",
    "# Show the plot with random images\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO object detection (Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6576,
     "status": "ok",
     "timestamp": 1733765789380,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "mSIVuur7HPAH",
    "outputId": "c3db7b35-5a5e-485a-8524-76e4b279e82d"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision pycocotools tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2875,
     "status": "ok",
     "timestamp": 1733765800101,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "lePEUJK3HSB-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Process annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, width, height = ann[\"bbox\"]\n",
    "            if width > 0 and height > 0:  # Only add valid boxes\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensor\n",
    "        if len(boxes) == 0:  # Handle no annotations\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2cgeox_g7tk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create datasets for train and validation\n",
    "train_annotation_file = 'archive/train_annotations1.json'\n",
    "val_annotation_file = 'archive/valid_annotations1.json'\n",
    "train_image_dir = 'archive/dataset/dataset/train/images'\n",
    "val_image_dir = 'archive/dataset/dataset/valid/images'\n",
    "\n",
    "train_dataset = COCODataset(train_annotation_file, train_image_dir)\n",
    "val_dataset = COCODataset(val_annotation_file, val_image_dir)\n",
    "\n",
    "# Create DataLoader for both train and validation datasets\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useless/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1325,
     "status": "ok",
     "timestamp": 1733765802904,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "mzA5uWIvJeMf",
    "outputId": "f65770e7-7887-44ff-8929-31f39107a926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define paths\n",
    "train_annotation_file = 'archive/train_annotations1.json'\n",
    "val_annotation_file = 'archive/valid_annotations1.json'\n",
    "train_image_dir = 'archive/dataset/dataset/train/images'\n",
    "val_image_dir = 'archive/dataset/dataset/valid/images'\n",
    "\n",
    "# Define transformations (Optional, can be expanded as needed)\n",
    "def get_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COCODataset(train_annotation_file, train_image_dir, transforms=get_transform())\n",
    "val_dataset = COCODataset(val_annotation_file, val_image_dir, transforms=get_transform())\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1733765814512,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "59Yo3fM1JpVf",
    "outputId": "4eebf0ab-6e40-4db8-8c06-bb210aef9f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "num_classes = 4  # 3 classes (pothole, cracks, open_manhole) + 1 background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1581,
     "status": "ok",
     "timestamp": 1733765816091,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "rsppeWJEcEkr",
    "outputId": "dcf089da-17f7-45ed-d19f-1bfc48b8b5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation successful!\n",
      "Data validation successful!\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def validate_data(image_dir, annotation_file):\n",
    "    # Load annotations from the JSON file\n",
    "    with open(annotation_file) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Get image file names from the annotation data\n",
    "    image_files = {image['file_name'] for image in annotations['images']}\n",
    "\n",
    "    # Check if image files exist\n",
    "    missing_images = []\n",
    "    for image_file in image_files:\n",
    "        if not os.path.exists(os.path.join(image_dir, image_file)):\n",
    "            missing_images.append(image_file)\n",
    "\n",
    "    if missing_images:\n",
    "        print(f\"Missing images: {missing_images}\")\n",
    "        return False\n",
    "\n",
    "    # Check if all bounding boxes are valid\n",
    "    invalid_bboxes = []\n",
    "    for annotation in annotations['annotations']:\n",
    "        x, y, w, h = annotation['bbox']\n",
    "        if w <= 0 or h <= 0:\n",
    "            invalid_bboxes.append(annotation['id'])\n",
    "\n",
    "    if invalid_bboxes:\n",
    "        print(f\"Invalid bounding boxes for annotations: {invalid_bboxes}\")\n",
    "        return False\n",
    "\n",
    "    print(\"Data validation successful!\")\n",
    "    return True\n",
    "\n",
    "# Define paths for your data\n",
    "train_annotation_file = 'archive/train_annotations1.json'\n",
    "val_annotation_file = 'archive/valid_annotations1.json'\n",
    "train_image_dir = 'archive/dataset/dataset/train/images'\n",
    "val_image_dir = 'archive/dataset/dataset/valid/images'\n",
    "\n",
    "# Validate train and validation data\n",
    "if not validate_data(train_image_dir, train_annotation_file):\n",
    "    print(\"Training data validation failed. Please check your images and annotations.\")\n",
    "elif not validate_data(val_image_dir, val_annotation_file):\n",
    "    print(\"Validation data validation failed. Please check your images and annotations.\")\n",
    "else:\n",
    "    print(\"Starting training...\")\n",
    "    # Proceed with training after successful data validation\n",
    "    model.train()  # Make sure your model is in training mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1733765816092,
     "user": {
      "displayName": "Sabid Rahman",
      "userId": "15766806607851674634"
     },
     "user_tz": -360
    },
    "id": "eUt37jjqJyF4"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum all losses in the loss_dict\n",
    "        if isinstance(loss_dict, list):\n",
    "            losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "        else:\n",
    "            losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "        losses.backward()  # Now losses is a tensor, so backward() can be called\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for images, targets in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Sum all losses in the loss_dict\n",
    "            if isinstance(loss_dict, list):\n",
    "                losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "            else:\n",
    "                losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "            epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hsEVDUYgwjP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/559 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# check if using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss = validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f\"faster_rcnn_epoch_{epoch + 1}.pth\")\n",
    "    print(f\"Model saved for epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T9eh4UFkfDD"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"/content/drive/MyDrive/data/data.yaml\",  # path to dataset YAML\n",
    "    epochs=25,  # number of training epochs\n",
    "    imgsz=224,  # training image size\n",
    "    device=\"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"/content/drive/MyDrive/data/dataset/test/images/images (1).jpeg\")\n",
    "results[0].show()\n",
    "\n",
    "# Export the model to ONNX format\n",
    "path = model.export(format=\"onnx\")  # return path to exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:04:29.552933Z",
     "start_time": "2024-12-12T19:51:00.772913Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"F:/data/data.yaml\",  # path to dataset YAML file\n",
    "    epochs=25,  # number of training epochs\n",
    "    imgsz=416,  # reduce image size from 224 to 416 or smaller\n",
    "    device=\"cuda\",  # device to run on, use \"cuda\" if you want to use GPU\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image (make sure the path is correct)\n",
    "results = model(\"F:/data/dataset/test/images/images (1).jpeg\")  # Adjust the path\n",
    "results[0].show()  # Show the results\n",
    "\n",
    "# Export the model to ONNX format\n",
    "path = model.export(format=\"onnx\")  # return path to exported model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:49:47.674607Z",
     "start_time": "2024-12-26T19:49:36.433792Z"
    }
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=C:/Users/Ahsan/runs/detect/train25/weights/best.pt conf=0.25 source=F:/data/dataset/test/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:34:34.491505Z",
     "start_time": "2024-12-12T20:34:34.049114Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the folder where YOLO saved the results\n",
    "results_path = \"runs/detect/predict6\"\n",
    "\n",
    "# Get the list of predicted images\n",
    "predicted_images = sorted(\n",
    "    [os.path.join(results_path, img) for img in os.listdir(results_path) if img.endswith(('.jpg', '.png'))]\n",
    ")\n",
    "\n",
    "# Set up the plot (1 row and as many columns as needed)\n",
    "fig, axes = plt.subplots(1, len(predicted_images), figsize=(20, 5))\n",
    "\n",
    "# Loop through the images and display them\n",
    "for i, image_path in enumerate(predicted_images):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "    # Display the image in the row\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')  # Hide axes\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:17:57.319273Z",
     "start_time": "2024-12-26T20:17:57.298329Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the Dataset class\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "        self.coco = COCO(annotation_file)  # Load the COCO annotations\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms  # Transformations to be applied to the image\n",
    "        self.image_ids = list(self.coco.imgs.keys())  # Get all image IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)  # Return the number of images in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]  # Get the image ID at the index\n",
    "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))  # Load annotations for the image\n",
    "        image_info = self.coco.loadImgs(image_id)[0]  # Get image info\n",
    "\n",
    "        # Load the image\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Process the annotations into bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, width, height = ann[\"bbox\"]\n",
    "            if width > 0 and height > 0:  # Only add valid boxes (non-zero size)\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensor\n",
    "        if len(boxes) == 0:  # Handle images with no annotations\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        # Apply transformations to the image, if any\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:18:06.123428Z",
     "start_time": "2024-12-26T20:18:06.110462Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation function\n",
    "def get_transform(train=True):\n",
    "    transform_list = []\n",
    "    if train:\n",
    "        # Omit RandomHorizontalFlip since augmentation is already done in preprocessing\n",
    "        pass\n",
    "    transform_list.append(transforms.ToTensor())  # Converts image to Tensor\n",
    "    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))  # Normalize the image\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T20:18:07.940724Z",
     "start_time": "2024-12-26T20:18:07.922772Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = [image.to(device) for image in images]  # Move images to device (GPU)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum all losses in the loss_dict\n",
    "        if isinstance(loss_dict, list):\n",
    "            losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "        else:\n",
    "            losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "        losses.backward()  # Now losses is a tensor, so backward() can be called\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "# Define the validation function\n",
    "import torch\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.train()  # Temporarily enable training mode to compute validation loss\n",
    "\n",
    "    total_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Check if the model returned a valid loss dictionary\n",
    "            if isinstance(loss_dict, dict):\n",
    "                batch_loss = sum(loss.item() for loss in loss_dict.values())\n",
    "            else:\n",
    "                raise ValueError(\"Model did not return a loss dictionary during validation.\")\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:25:54.703262Z",
     "start_time": "2024-12-26T20:18:15.905635Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_images_path = \"F:/data/dataset/train/images\"\n",
    "valid_images_path = \"F:/data/dataset/valid/images\"\n",
    "train_annotations_path = \"F:/data/train_annotations1.json\"\n",
    "valid_annotations_path = \"F:/data/valid_annotations1.json\"\n",
    "\n",
    "train_dataset = COCODataset(train_annotations_path, train_images_path, transforms=get_transform(train=True))\n",
    "val_dataset = COCODataset(valid_annotations_path, valid_images_path, transforms=get_transform(train=False))\n",
    "\n",
    "# Create DataLoader for both train and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Initialize the model, optimizer, and learning rate scheduler\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Ensure the model directory exists\n",
    "os.makedirs(\"F:/data/model_cnn/faster_r_cnn_1\", exist_ok=True)\n",
    "\n",
    "# Create a CSV file to store mAP values\n",
    "with open(\"F:/data/model_cnn/faster_r_cnn_1/map_values.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"Epoch\", \"mAP\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss = validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    save_path = f\"F:/data/model_cnn/faster_r_cnn_1/faster_rcnn_epoch_{epoch + 1}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:01:19.043895Z",
     "start_time": "2024-12-15T09:00:37.472758Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = [image.to(device) for image in images]  # Move images to device (GPU)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum all losses in the loss_dict\n",
    "        if isinstance(loss_dict, list):\n",
    "            losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "        else:\n",
    "            losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "        losses.backward()  # Now losses is a tensor, so backward() can be called\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "# Define the validation function\n",
    "import torch\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.train()  # Temporarily enable training mode to compute validation loss\n",
    "\n",
    "    total_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Check if the model returned a valid loss dictionary\n",
    "            if isinstance(loss_dict, dict):\n",
    "                batch_loss = sum(loss.item() for loss in loss_dict.values())\n",
    "            else:\n",
    "                raise ValueError(\"Model did not return a loss dictionary during validation.\")\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:20:29.837273Z",
     "start_time": "2024-12-15T09:15:23.194465Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import AveragePrecision, ConfusionMatrix, F1Score\n",
    "import numpy as np\n",
    "\n",
    "# ... (Your COCODataset and get_transform definitions) ...\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_images_path = \"F:/data/dataset/train/images\"\n",
    "valid_images_path = \"F:/data/dataset/valid/images\"\n",
    "train_annotations_path = \"F:/data/train_annotations.json\"\n",
    "valid_annotations_path = \"F:/data/valid_annotations.json\"\n",
    "\n",
    "train_dataset = COCODataset(train_annotations_path, train_images_path, transforms=get_transform(train=True))\n",
    "val_dataset = COCODataset(valid_annotations_path, valid_images_path, transforms=get_transform(train=False))\n",
    "\n",
    "# Create DataLoader for both train and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Initialize the model, optimizer, and learning rate scheduler\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Ensure the model directory exists\n",
    "os.makedirs(\"F:/data/model_cnn/faster_r_cnn_1/results\", exist_ok=True)\n",
    "\n",
    "# Create a CSV file to store results\n",
    "with open(\"F:/data/model_cnn/faster_r_cnn_1/results/results.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"Epoch\", \"Train Loss\", \"Validation Loss\", \"mAP\", \"F1 Score\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss, mAP, conf_matrix, f1_score = validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, mAP: {mAP:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    writer.writerow({\"Epoch\": epoch + 1, \"Train Loss\": train_loss, \"Validation Loss\": val_loss, \"mAP\": mAP, \"F1 Score\": f1_score})\n",
    "\n",
    "    # Save model checkpoint\n",
    "    save_path = f\"F:/data/model_cnn/faster_r_cnn_1/faster_rcnn_epoch_{epoch + 1}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved for epoch {epoch + 1} at {save_path}\")\n",
    "\n",
    "    # Save confusion matrix\n",
    "    np.save(f\"F:/data/model_cnn/faster_r_cnn_1/results/confusion_matrix_epoch_{epoch + 1}.npy\", conf_matrix.cpu().numpy())\n",
    "\n",
    "# Define the training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = [image.to(device) for image in images]  # Move images to device (GPU)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum all losses in the loss_dict\n",
    "        if isinstance(loss_dict, list):\n",
    "            losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "        else:\n",
    "            losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "        losses.backward()  # Now losses is a tensor, so backward() can be called\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "# Define the validation function\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    evaluator = AveragePrecision(iou_threshold=0.5)\n",
    "    conf_mat = ConfusionMatrix(num_classes=len(your_class_labels))\n",
    "    f1_score = F1Score(num_classes=len(your_class_labels), average='macro')  # Calculate macro-averaged F1-score\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)[0]  # Get predictions (assuming model output is a tuple)\n",
    "            pred_labels = [torch.argmax(p['scores'], dim=0).item() for p in predictions]\n",
    "            true_labels = [t['labels'].cpu().numpy() for t in targets]\n",
    "\n",
    "            # Calculate loss (unchanged)\n",
    "            if isinstance(loss_dict, dict):\n",
    "                batch_loss = sum(loss.item() for loss in loss_dict.values())\n",
    "            else:\n",
    "                raise ValueError(\"Model did not return a loss dictionary during validation.\")\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            # Update evaluators\n",
    "            evaluator.update(predictions, targets)\n",
    "            conf_mat.update(torch.tensor(pred_labels), torch.tensor(true_labels))\n",
    "            f1_score.update(torch.tensor(pred_labels), torch.tensor(true_labels))\n",
    "\n",
    "    # Calculate and return metrics\n",
    "    mAP = evaluator.compute()\n",
    "    conf_matrix = conf_mat.compute()\n",
    "    f1_score = f1_score.compute()\n",
    "    return total_loss / len(data_loader), mAP, conf_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T20:09:06.729729Z",
     "start_time": "2024-12-14T20:09:06.717761Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T10:16:37.979429Z",
     "start_time": "2024-12-15T10:16:31.239061Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Paths\n",
    "test_images_path = \"F:/data/dataset/test/images\"\n",
    "results_path = \"F:/data/model_cnn/faster_r_cnn_1/results\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the trained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)  # Ensure pretrained=False since it's a custom-trained model\n",
    "model.load_state_dict(torch.load('F:/data/model_cnn/faster_r_cnn_1/faster_rcnn_epoch_10.pth'))  # Update with your trained model path\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Threshold for filtering low-confidence predictions\n",
    "threshold = 0.5\n",
    "\n",
    "# Function to test and save results\n",
    "def test_and_save_results(image_path, output_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "\n",
    "    # Extract predictions\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "    # Filter predictions by threshold\n",
    "    filtered_boxes = boxes[scores >= threshold]\n",
    "    filtered_labels = labels[scores >= threshold]\n",
    "    filtered_scores = scores[scores >= threshold]\n",
    "\n",
    "    # Plot the image with bounding boxes\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                  linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min, f\"{label} ({score:.2f})\",\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5), fontsize=12, color='black')\n",
    "\n",
    "    # Save the output\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Test all images in the folder\n",
    "for image_name in os.listdir(test_images_path):\n",
    "    image_path = os.path.join(test_images_path, image_name)\n",
    "    if os.path.isfile(image_path) and image_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "        output_path = os.path.join(results_path, f\"results_{image_name}\")\n",
    "        print(f\"Processing: {image_name}\")\n",
    "        test_and_save_results(image_path, output_path)\n",
    "\n",
    "print(f\"All results saved in {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T08:02:19.477086Z",
     "start_time": "2024-12-15T08:02:17.426569Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Check and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)  # Custom-trained model, no pretrained weights\n",
    "model.load_state_dict(torch.load('F:/data/model_cnn/faster_r_cnn/faster_rcnn_epoch_10.pth', map_location=device))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)  # Move model to the device\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"F:/data/dataset/test/images/images (5).jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Open the image in RGB mode\n",
    "\n",
    "# Convert the image to tensor\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)  # Add batch dimension and move to the device\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "# Extract predictions\n",
    "boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "labels = prediction[0]['labels'].cpu().numpy()\n",
    "scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Set a confidence threshold\n",
    "threshold = 0.5\n",
    "filtered_boxes = boxes[scores >= threshold]\n",
    "filtered_labels = labels[scores >= threshold]\n",
    "filtered_scores = scores[scores >= threshold]\n",
    "\n",
    "# Plot the image with bounding boxes\n",
    "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "ax.imshow(image)\n",
    "\n",
    "# Draw bounding boxes and labels\n",
    "for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
    "    x_min, y_min, x_max, y_max = box  # Coordinates of the bounding box\n",
    "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x_min, y_min, f\"{label} ({score:.2f})\", bbox=dict(facecolor='yellow', alpha=0.5), fontsize=12, color='black')\n",
    "\n",
    "# Save and display the result\n",
    "output_path = \"F:/data/dataset/test/images/results/p_images (5).jpeg\"\n",
    "plt.savefig(output_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:26:00.044218Z",
     "start_time": "2024-12-26T18:40:44.170869Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=r\"F:/data/data.yaml\",  # Path to dataset YAML\n",
    "    epochs=50,                  # Number of training epochs\n",
    "    imgsz=640,                  # Training image size (default is 640 for YOLOv8)\n",
    "    device=\"gpu\"                # Device to run on (CPU or GPU)\n",
    ")\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(r\"F:/data/dataset/test/images/images (1).jpeg\")\n",
    "results[0].show()  # Display the first result\n",
    "\n",
    "# Export the model to ONNX format\n",
    "path = model.export(format=\"onnx\")  # Save the model in ONNX format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:45:17.021352Z",
     "start_time": "2024-12-26T19:45:08.423594Z"
    }
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=C:\\Users\\Ahsan\\runs\\detect\\train25\\weights\\best.onnx conf=0.25 source=F:\\data\\dataset\\test\\images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T19:42:56.420119Z",
     "start_time": "2024-12-26T19:42:56.114411Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from models import YOLO  # Assuming your YOLO model is defined in 'models.py'\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO()  # Replace with your model's class\n",
    "model.load_state_dict(torch.load('C:/Users/Ahsan/runs/detect/train25/weights/best.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to the test images directory\n",
    "test_dir = 'data/dataset/test/images'\n",
    "\n",
    "# Iterate through each image in the test directory\n",
    "for image_path in os.listdir(test_dir):\n",
    "    image = Image.open(os.path.join(test_dir, image_path))\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        detections = model(image_tensor)\n",
    "\n",
    "    # Process detections (e.g., filter by confidence, draw bounding boxes)\n",
    "    # ...\n",
    "\n",
    "    # Optionally, save or display the results\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:41:38.545953Z",
     "start_time": "2024-12-26T21:41:37.625781Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from IPython.display import Video  # For displaying the video in Colab\n",
    "\n",
    "# Load your YOLO model\n",
    "model = YOLO('F:/data/model_cnn/faster_r_cnn_1/faster_rcnn_epoch_10.pth')\n",
    "\n",
    "# Path to your input video\n",
    "video_path = 'F:/data/dataset/test_video/testvideo1.mp4'  # Replace with your video file path\n",
    "output_path = 'F:/data/dataset/test_video/results/testvideo_cnn1.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save the annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Annotate the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Display the processed video in Colab\n",
    "print(f\"Processed video saved at: {output_path}\")\n",
    "Video(output_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:53:08.656077Z",
     "start_time": "2024-12-26T21:52:11.818293Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "from IPython.display import Video  # For displaying the video in Colab\n",
    "\n",
    "# Load your Faster R-CNN model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=None)  # No pre-trained weights\n",
    "model.load_state_dict(torch.load('F:/data/model_cnn/faster_r_cnn_1/faster_rcnn_epoch_10.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Path to your input video\n",
    "video_path = 'F:/data/dataset/test_video/testvideo7.mp4'  # Replace with your video file path\n",
    "output_path = 'F:/data/dataset/test_video/results/testvideo_cnn7.mp4'\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save the annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame (NumPy array) to PIL image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "\n",
    "    # Apply the same transformations as in training\n",
    "    transform = get_transform(train=False)  # Use the same transform as validation\n",
    "    pil_image = transform(pil_image).unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "\n",
    "    # Perform detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model(pil_image)  # Get predictions (boxes, labels, scores)\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "    # Iterate over the detections and draw boxes\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.5:  # Filter out weak detections\n",
    "            x1, y1, x2, y2 = box\n",
    "            frame = cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            label_text = f\"Label: {label}, Score: {score:.2f}\"\n",
    "            frame = cv2.putText(frame, label_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Display the processed video in Colab\n",
    "print(f\"Processed video saved at: {output_path}\")\n",
    "Video(output_path, embed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T21:30:00.648317Z",
     "start_time": "2024-12-29T20:40:59.572101Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "\n",
    "# Load COCO annotations\n",
    "def load_coco_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# COCO Data Generator\n",
    "class CocoDataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, annotations, batch_size, target_size):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = annotations['annotations']\n",
    "        self.images = {img['id']: img for img in annotations['images']}\n",
    "        self.categories = {cat['id']: cat for cat in annotations['categories']}\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.num_samples = len(self.annotations)\n",
    "        self.indices = list(range(self.num_samples))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        images = []\n",
    "        labels = []\n",
    "        for idx in batch_indices:\n",
    "            ann = self.annotations[idx]\n",
    "            img_info = self.images[ann['image_id']]\n",
    "            img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Image not found: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            img = img / 255.0\n",
    "\n",
    "            # Generate one-hot label\n",
    "            label = np.zeros(len(self.categories))\n",
    "            label[ann['category_id'] - 1] = 1  # Category IDs in annotations start from 1\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Directories and parameters\n",
    "train_annotations_file = \"F:/data/train_annotations1.json\"\n",
    "valid_annotations_file = \"F:/data/valid_annotations1.json\"\n",
    "train_image_dir = \"F:/data/dataset/train/images\"\n",
    "valid_image_dir = \"F:/data/dataset/valid/images\"\n",
    "target_size = (224, 224)\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "\n",
    "# Load annotations\n",
    "train_annotations = load_coco_annotations(train_annotations_file)\n",
    "valid_annotations = load_coco_annotations(valid_annotations_file)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = CocoDataGenerator(train_image_dir, train_annotations, batch_size, target_size)\n",
    "valid_generator = CocoDataGenerator(valid_image_dir, valid_annotations, batch_size, target_size)\n",
    "\n",
    "# Load VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(valid_generator)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T22:20:00.194854Z",
     "start_time": "2024-12-29T21:31:20.721881Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import cv2\n",
    "\n",
    "# Load COCO annotations\n",
    "def load_coco_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# COCO Data Generator\n",
    "class CocoDataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, annotations, batch_size, target_size):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = annotations['annotations']\n",
    "        self.images = {img['id']: img for img in annotations['images']}\n",
    "        self.categories = {cat['id']: cat for cat in annotations['categories']}\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.num_samples = len(self.annotations)\n",
    "        self.indices = list(range(self.num_samples))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        images = []\n",
    "        labels = []\n",
    "        for idx in batch_indices:\n",
    "            ann = self.annotations[idx]\n",
    "            img_info = self.images[ann['image_id']]\n",
    "            img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Image not found: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.target_size)\n",
    "            img = img / 255.0\n",
    "\n",
    "            # Generate one-hot label\n",
    "            label = np.zeros(len(self.categories))\n",
    "            label[ann['category_id'] - 1] = 1  # Category IDs in annotations start from 1\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Directories and parameters\n",
    "train_annotations_file = r\"archive\\train_annotations1.json\"\n",
    "valid_annotations_file = r\"archive\\valid_annotations1.json\"\n",
    "train_image_dir = r\"archive\\dataset\\dataset\\train\\images\"\n",
    "valid_image_dir = r\"archive\\dataset\\dataset\\valid\\images\"\n",
    "target_size = (224, 224)\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "\n",
    "# Load annotations\n",
    "train_annotations = load_coco_annotations(train_annotations_file)\n",
    "valid_annotations = load_coco_annotations(valid_annotations_file)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = CocoDataGenerator(train_image_dir, train_annotations, batch_size, target_size)\n",
    "valid_generator = CocoDataGenerator(valid_image_dir, valid_annotations, batch_size, target_size)\n",
    "\n",
    "# Load VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "checkpoint = ModelCheckpoint('archive\\dataset\\dataset/vgg/best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model with the checkpoint callback\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(valid_generator),\n",
    "    callbacks=[checkpoint]  # Add the checkpoint callback here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-29T22:21:38.574782Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from IPython.display import Video  # For displaying the video in Colab\n",
    "\n",
    "# Load the VGG16 model\n",
    "vgg_model_path = 'archive\\dataset\\dataset/vgg/best_model.keras'  # Path to your saved VGG model\n",
    "model = load_model(vgg_model_path)\n",
    "\n",
    "# Path to your input video\n",
    "video_path  = os.path.join('archive','dataset','dataset','test','video','testvideo1.mp4')\n",
    "output_path = os.path.join('archive','dataset','dataset','test','video','results','testvideo_vgg1.mp4')\n",
    "\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define video writer to save the annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Define the categories\n",
    "categories = ['pothole', 'cracks', 'open_manhole']  # Assuming you have 3 categories\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame (NumPy array) to PIL image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "\n",
    "    # Resize and preprocess the image for VGG16\n",
    "    img = pil_image.resize((224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize the image\n",
    "\n",
    "    # Perform prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    predicted_label = categories[predicted_class[0]]\n",
    "\n",
    "    # Draw label on the frame\n",
    "    label_text = f\"Prediction: {predicted_label}, Score: {prediction[0][predicted_class[0]]:.2f}\"\n",
    "    frame = cv2.putText(frame, label_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Display the processed video in Colab\n",
    "print(f\"Processed video saved at: {output_path}\")\n",
    "Video(output_path, embed=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fintune Classification model (Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T19:19:57.302061Z",
     "start_time": "2024-12-30T19:19:47.295899Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:27:07.402925Z",
     "start_time": "2024-12-30T20:27:01.829942Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        with open(annotations_file) as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in self.coco_data[\"images\"]}\n",
    "        self.annotations = self.coco_data[\"annotations\"]\n",
    "        self.categories = {cat[\"id\"]: cat[\"name\"] for cat in self.coco_data[\"categories\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        img_id = annotation[\"image_id\"]\n",
    "        category_id = annotation[\"category_id\"]\n",
    "\n",
    "        # Load the image\n",
    "        img_path = f\"{self.img_dir}/{self.image_id_to_filename[img_id]}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image and label (0-based index)\n",
    "        return image, category_id - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:27:08.603909Z",
     "start_time": "2024-12-30T20:27:08.531581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "train_annotations_file = os.path.join(\"archive\", \"train_annotations1.json\")\n",
    "valid_annotations_file = os.path.join(\"archive\", \"valid_annotations1.json\")\n",
    "train_image_dir = os.path.join(\"archive\", \"dataset\", \"dataset\", \"train\", \"images\")\n",
    "valid_image_dir = os.path.join(\"archive\", \"dataset\", \"dataset\", \"valid\", \"images\")\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = COCODataset(train_annotations_file, train_image_dir, transform=transform)\n",
    "valid_dataset = COCODataset(valid_annotations_file, valid_image_dir, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset size: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:27:11.808859Z",
     "start_time": "2024-12-30T20:27:11.019831Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Load EfficientNet model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=3)  # Adjust num_classes to match your categories\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:46:51.817913Z",
     "start_time": "2024-12-30T20:27:13.374405Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(25):  # Adjust epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:46:54.430894Z",
     "start_time": "2024-12-30T20:46:54.291958Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Load EfficientNet model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=3)  # Adjust num_classes to match your categories\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:47:09.163632Z",
     "start_time": "2024-12-30T20:46:56.317015Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:49:02.642176Z",
     "start_time": "2024-12-30T20:49:02.514277Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"archive\\dataset\\dataset\\efficientnet/model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference using the trained model\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Load the trained model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=3)  # Adjust num_classes to match your categories\n",
    "model.load_state_dict(torch.load(\"archive\\dataset\\dataset\\efficientnet/model.pth\"))\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Function to predict the class of an image\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    return predicted.item()\n",
    "# Example usage\n",
    "image_path = os.path.join(base_dir, \"dataset\", \"dataset\", \"test\", \"images\", \"360_F_916445076_KAGVDE2nuHOIxNARmepMJTgHQqd9Y7y0.jpg\")  # Replace with your image path\n",
    "predicted_class = predict_image(image_path)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification (Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:20:25.017158Z",
     "start_time": "2024-12-30T21:20:16.609743Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet-pytorch\n",
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:20:31.001041Z",
     "start_time": "2024-12-30T21:20:30.990071Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, images_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (string): Path to the annotations file (COCO format).\n",
    "            images_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        with open(annotations_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Map category id to label (adjust if you have different categories)\n",
    "        self.category_map = {\n",
    "            1: \"pothole\",\n",
    "            2: \"cracks\",\n",
    "            3: \"open_manhole\"\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image information\n",
    "        image_info = self.annotations['images'][idx]\n",
    "        image_id = image_info['id']\n",
    "        image_filename = image_info['file_name']\n",
    "        image_path = os.path.join(self.images_dir, image_filename)\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get the annotations for the image\n",
    "        annotations = [anno for anno in self.annotations['annotations'] if anno['image_id'] == image_id]\n",
    "\n",
    "        # Prepare the label (class id)\n",
    "        labels = []\n",
    "        for anno in annotations:\n",
    "            category_id = anno['category_id']\n",
    "            labels.append(category_id - 1)  # Convert to 0-indexed class\n",
    "\n",
    "        # If you have multiple annotations, pick the first one (or handle it based on your logic)\n",
    "        label = labels[0] if len(labels) > 0 else 0\n",
    "\n",
    "        # Apply the transformation if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:20:41.314577Z",
     "start_time": "2024-12-30T21:20:41.280667Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "# Define data transformations for training and validation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize image to match EfficientNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean and std\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "base_dir = 'archive'\n",
    "\n",
    "# Define Dataset and DataLoader\n",
    "train_dataset = CustomDataset(\n",
    "    annotations_file=os.path.join(base_dir, 'train_annotations1.json'),\n",
    "    images_dir      =os.path.join(base_dir, 'dataset', 'dataset', 'train', 'images'),\n",
    "    transform       =transform_train\n",
    ")\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    annotations_file=os.path.join(base_dir, 'valid_annotations1.json'),\n",
    "    images_dir      =os.path.join(base_dir, 'dataset', 'dataset', 'valid', 'images'),\n",
    "    transform       =transform_valid\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:20:47.433695Z",
     "start_time": "2024-12-30T21:20:47.265113Z"
    }
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained EfficientNet model (EfficientNet-B0 in this case)\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Modify the classifier layer to match your number of classes\n",
    "num_classes = 3  # You have 3 classes: pothole, cracks, open_manhole\n",
    "model._fc = nn.Linear(model._fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:27:26.102902Z",
     "start_time": "2024-12-30T21:20:58.567185Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "# Define the optimizer (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in valid_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_total += targets.size(0)\n",
    "            valid_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    print(f\"Validation Accuracy: {valid_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the torch version\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:30:02.212733Z",
     "start_time": "2024-12-30T21:30:02.114478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = os.path.join('archive\\dataset\\dataset\\efficientnet', 'efficientnet_model.pth')\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:32:23.788228Z",
     "start_time": "2024-12-30T21:32:23.775747Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:32:30.127748Z",
     "start_time": "2024-12-30T21:32:29.834988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model (assuming you are using EfficientNet-B0, change if necessary)\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=3)\n",
    "\n",
    "# Load the trained weights\n",
    "model_path = 'archive\\dataset\\dataset\\efficientnet\\efficientnet_model.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:32:42.523025Z",
     "start_time": "2024-12-30T21:32:42.511054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transformation for the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to the size expected by EfficientNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization for EfficientNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:32:47.748401Z",
     "start_time": "2024-12-30T21:32:47.733928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "class_names = ['pothole', 'cracks', 'open_manhole']  # Update based on your dataset\n",
    "\n",
    "def classify_image(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Apply the transformations\n",
    "    img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Predict with the model\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        output = model(img)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    # Return the predicted class label\n",
    "    return class_names[predicted_class.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:32:56.150769Z",
     "start_time": "2024-12-30T21:32:53.603660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory containing the test images\n",
    "test_dir = 'archive/dataset/dataset/test/images'\n",
    "\n",
    "# List all the images in the directory\n",
    "test_images = [f for f in os.listdir(test_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Iterate over all test images and classify them\n",
    "for image_name in test_images:\n",
    "    image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "    # Classify the image\n",
    "    predicted_class = classify_image(image_path)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Image: {image_name} - Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
