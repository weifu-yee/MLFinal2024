{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6820301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declarate Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Process annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, width, height = ann[\"bbox\"]\n",
    "            if width > 0 and height > 0:  # Only add valid boxes\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Convert to tensor\n",
    "        if len(boxes) == 0:  # Handle no annotations\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a7cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "## Declarate DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define paths\n",
    "train_annotation_file = 'archive/train_annotations1.json'\n",
    "val_annotation_file = 'archive/valid_annotations1.json'\n",
    "train_image_dir = 'archive/dataset/dataset/train/images'\n",
    "val_image_dir = 'archive/dataset/dataset/valid/images'\n",
    "\n",
    "# Define transformations (Optional, can be expanded as needed)\n",
    "def get_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COCODataset(train_annotation_file, train_image_dir, transforms=get_transform())\n",
    "val_dataset = COCODataset(val_annotation_file, val_image_dir, transforms=get_transform())\n",
    "\n",
    "# DataLoaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee0df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Loading the pre-trained Faster R-CNN model\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "num_classes = 4  # 3 classes (pothole, cracks, open_manhole) + 1 background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eea907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation successful!\n",
      "Data validation successful!\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "## Validata data\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def validate_data(image_dir, annotation_file):\n",
    "    # Load annotations from the JSON file\n",
    "    with open(annotation_file) as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Get image file names from the annotation data\n",
    "    image_files = {image['file_name'] for image in annotations['images']}\n",
    "\n",
    "    # Check if image files exist\n",
    "    missing_images = []\n",
    "    for image_file in image_files:\n",
    "        if not os.path.exists(os.path.join(image_dir, image_file)):\n",
    "            missing_images.append(image_file)\n",
    "\n",
    "    if missing_images:\n",
    "        print(f\"Missing images: {missing_images}\")\n",
    "        return False\n",
    "\n",
    "    # Check if all bounding boxes are valid\n",
    "    invalid_bboxes = []\n",
    "    for annotation in annotations['annotations']:\n",
    "        x, y, w, h = annotation['bbox']\n",
    "        if w <= 0 or h <= 0:\n",
    "            invalid_bboxes.append(annotation['id'])\n",
    "\n",
    "    if invalid_bboxes:\n",
    "        print(f\"Invalid bounding boxes for annotations: {invalid_bboxes}\")\n",
    "        return False\n",
    "\n",
    "    print(\"Data validation successful!\")\n",
    "    return True\n",
    "\n",
    "# Define paths for your data\n",
    "train_annotation_file = 'archive/train_annotations1.json'\n",
    "val_annotation_file = 'archive/valid_annotations1.json'\n",
    "train_image_dir = 'archive/dataset/dataset/train/images'\n",
    "val_image_dir = 'archive/dataset/dataset/valid/images'\n",
    "\n",
    "# Validate train and validation data\n",
    "if not validate_data(train_image_dir, train_annotation_file):\n",
    "    print(\"Training data validation failed. Please check your images and annotations.\")\n",
    "elif not validate_data(val_image_dir, val_annotation_file):\n",
    "    print(\"Validation data validation failed. Please check your images and annotations.\")\n",
    "else:\n",
    "    print(\"Starting training...\")\n",
    "    # Proceed with training after successful data validation\n",
    "    model.train()  # Make sure your model is in training mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a978ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define one epoch training and validation functions\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Sum all losses in the loss_dict\n",
    "        if isinstance(loss_dict, list):\n",
    "            losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "        else:\n",
    "            losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "        losses.backward()  # Now losses is a tensor, so backward() can be called\n",
    "        optimizer.step()  # Update the weights\n",
    "        epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for images, targets in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Sum all losses in the loss_dict\n",
    "            if isinstance(loss_dict, list):\n",
    "                losses = sum(loss for loss in loss_dict)  # Don't use .item(), keep it as a tensor\n",
    "            else:\n",
    "                losses = sum(loss for loss in loss_dict.values())  # Same here\n",
    "\n",
    "            epoch_loss += losses.item()  # Add the loss to the epoch's loss (convert to float here for reporting)\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declarate optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3705431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 559/559 [57:19<00:00,  6.15s/it]  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m train_loss = train_one_epoch(model, optimizer, train_loader, device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m val_loss = \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Step the parameters\u001b[39;00m\n\u001b[32m     14\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mvalidate_one_epoch\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Sum all losses in the loss_dict\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_dict, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     losses = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss_dict\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Don't use .item(), keep it as a tensor\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     41\u001b[39m     losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())  \u001b[38;5;66;03m# Same here\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "# check if using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss = validate_one_epoch(model, val_loader, device)\n",
    "\n",
    "    # Step the parameters\n",
    "    optimizer.step()\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Step the learning rate scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f\"faster_rcnn_epoch_{epoch + 1}.pth\")\n",
    "    print(f\"Model saved for epoch {epoch + 1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
